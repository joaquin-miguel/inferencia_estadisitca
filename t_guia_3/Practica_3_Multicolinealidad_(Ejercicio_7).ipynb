
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CYkDNoc76nRj"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import statsmodels.api as sm\n",
        "import numpy as np\n",
        "from scipy import stats\n",
        "from scipy.optimize import minimize\n",
        "from numpy.linalg import inv\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from tqdm import tqdm  # Para barra de progreso"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multicolinealidad en Regresi√≥n Lineal\n"
      ],
      "metadata": {
        "id": "YHxhmPowNg3s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ejercicio 7\n",
        "\n",
        "\n",
        "Vamos a generar un conjunto de datos sint√©ticos que presenten multicolinealidad entre predictores."
      ],
      "metadata": {
        "id": "rVxSLL7HRiyE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generar `n = 100` samples de $X_1 ~ U(0, 1)$, de `Z ~ N(0, 0.1)` y de `Œµ ~ Normal(0, 1)`; a partir de ellas generar $n$ samples de $X_2 := 0.5X_1 + Z$ y de $$Y := 2 + 2X_1 + 0.3X_2 + Œµ$$\n",
        "\n"
      ],
      "metadata": {
        "id": "wQaokiAaOFzB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Generaci√≥n de datos**"
      ],
      "metadata": {
        "id": "j-rbWBW3Tdr1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# N√∫mero de muestras\n",
        "n = 100\n",
        "\n",
        "# variables\n",
        "X1 = stats.uniform.rvs(size=n)\n",
        "Z = stats.norm.rvs(loc = 0, scale = 0.1, size = n)\n",
        "e = stats.norm.rvs(size = n)\n",
        "X2 = 0.5*X1 + Z\n",
        "\n",
        "Y = 2 + 2*X1 + 0.3*X2 + e"
      ],
      "metadata": {
        "id": "vnDcQ97RjI0K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creaci√≥n del DataFrame\n",
        "data = pd.DataFrame({'X1': X1, 'X2': X2, 'Y': Y})\n",
        "data.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "YVAmutvNGTpX",
        "outputId": "afe5e46f-7aa3-41a9-c51b-ea992ea9c94b"
      },
      "execution_count": null
      
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Calcular la correlacion entre las muestras de $X_1$ y $X_2$ y graficar su distribucion conjunta."
      ],
      "metadata": {
        "id": "WHoj3FAKjBTu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# C√°lculo de la correlaci√≥n\n",
        "corr = stats.pearsonr(X1,X2).statistic\n",
        "print(corr)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WHJ3fQ4uVXAU",
        "outputId": "9025e816-e82a-412d-ebd7-7e2b3066a3e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8017354464093105\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(6, 4))\n",
        "#plt.scatter(X1, X2)\n",
        "sns.scatterplot(x='X1', y='X2', data=data, color='#2ca02c', s=40, alpha=0.7)\n",
        "plt.title('Distribuci√≥n Conjunta de X1 y X2', fontsize=12)\n",
        "plt.xlabel('X1', fontsize=10)\n",
        "plt.ylabel('X2', fontsize=10)\n",
        "plt.grid(True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "id": "R5MjTB_OU1W-",
        "outputId": "f2c7e549-fe77-47a2-e3af-ccba8ed9ccac"
      },
      "execution_count": null
  
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hay una alta correlaci√≥n entre las muestras de X1 y X2 ya que X2 est√° definida como una funci√≥n lineal de X1 m√°s un ruido.\n",
        "\n",
        "El gr√°fico muestra una nube de puntos con una clara forma lineal debido a la relaci√≥n $X_2=0.5X_1+Z$. A pesar del ruido Z, la relaci√≥n lineal subyacente sigue siendo visible.\n",
        "\n",
        "Esta correlaci√≥n ilustra el concepto de multicolinealidad, que ocurre cuando dos o m√°s predictores est√°n linealmente relacionados y tiene implicaciones importantes para la interpretaci√≥n de los coeficientes en modelos de regresi√≥n."
      ],
      "metadata": {
        "id": "aLEtPg7_jdtR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (b): Ajuste del modelo de regresi√≥n m√∫ltiple"
      ],
      "metadata": {
        "id": "lSaioIYVI7rE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Construimos manualmente la matriz de dise√±o (`X_intercept`) con una columna de unos (intercepto) y los predictores X1, X2.\n",
        "\n",
        "* Aplicamos la f√≥rmula OLS directamente: $$Œ≤=(X^T X)^{‚àí1} X^T Y$$\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "NY2X0zo2XWnQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_intercept = np.column_stack((np.ones_like(Y), X1,X2))\n",
        "beta_hat = np.linalg.inv(X_intercept.T @ X_intercept) @ X_intercept.T @ Y\n",
        "print('betas originales = ', [2,2,0.3])\n",
        "print('betas obtenidos = ', beta_hat)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TpVg8ECQWO2S",
        "outputId": "a87da955-b087-4e0c-b1f4-c33674dda2d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "betas originales =  [2, 2, 0.3]\n",
            "betas obtenidos =  [2.19645127 1.32357321 0.78302306]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "La multicolinealidad hace que sea dif√≠cil separar el efecto de $X_1$ y $X_2$ sobre $Y$.\n",
        "Es algo que surge de c√≥mo funciona la regresi√≥n m√∫ltiple, la multicolinealidad en s√≠ no tiene nada de malo. De hecho, funciona bien para predecir. Pero dificulta la interpretaci√≥n.\n",
        "La predicci√≥n de $Y$ puede seguir siendo precisa pero los coeficientes individuales pierden interpretaci√≥n directa\n",
        "\n"
      ],
      "metadata": {
        "id": "OBv8cFztjsiv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (c): Ajustes de modelos individuales"
      ],
      "metadata": {
        "id": "BE_giVPEI-Bx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Utilizando solo las muestras de $X_1$**"
      ],
      "metadata": {
        "id": "nPdK3TWkZ5F-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Solo con X1\n",
        "X_intercept = np.column_stack((np.ones_like(Y), X1))\n",
        "beta_hat = np.linalg.inv(X_intercept.T @ X_intercept) @ X_intercept.T @ Y\n",
        "print('betas originales = ', [2,2])\n",
        "print('betas obtenidos = ', beta_hat)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KBgQtlGfYsch",
        "outputId": "92a5236d-2ba9-4157-9a06-eb9a074efd30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "betas originales =  [2, 2]\n",
            "betas obtenidos =  [2.1851586 1.7194104]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Utilizando solo las muestras de $X_2$**"
      ],
      "metadata": {
        "id": "xCXsxp7eZ94f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_intercept = np.column_stack((np.ones_like(Y), X2))\n",
        "beta_hat = np.linalg.inv(X_intercept.T @ X_intercept) @ X_intercept.T @ Y\n",
        "print('betas originales = ', [2,0.3])\n",
        "print('betas obtenidos = ', beta_hat)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7h6aMFmNZ__o",
        "outputId": "994e8ca7-64b5-4057-cf40-0db9fa87778b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "betas originales =  [2, 0.3]\n",
            "betas obtenidos =  [2.44961463 2.46596099]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "$\\hat{\\beta_1}$ y $\\hat{\\beta_2}$ no son iguales a los coeficientes verdaderos, ya que parte de la informaci√≥n de $X_2$ se incorpora a $\\hat{\\beta_1}$ y viceversa."
      ],
      "metadata": {
        "id": "bZpceU3nj-WQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## (d)"
      ],
      "metadata": {
        "id": "IH-C-niVT7WQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(d) Repetir el ajuste del modelo 1000 veces con diferentes muestras y graficar la distribuci√≥n conjunta de\n",
        "ùõΩ\n",
        "^\n",
        "1  y\n",
        "ùõΩ\n",
        "^\n",
        "2."
      ],
      "metadata": {
        "id": "EYdgrjb0Gk1A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Simulaci√≥n para analizar variabilidad de coeficientes\n",
        "beta_1 = []\n",
        "beta_2 = []\n",
        "n = 100\n",
        "\n",
        "for _ in range(1000):\n",
        "    X1 = stats.uniform.rvs(size=n)\n",
        "    Z = stats.norm.rvs(loc = 0, scale = 0.1, size = n)\n",
        "    e = stats.norm.rvs(size = n)\n",
        "    X2 = 0.5*X1 + Z\n",
        "    Y = 2 + 2*X1 + 0.3*X2 + e\n",
        "\n",
        "\n",
        "    # Ajustar el modelo con X1 y X2\n",
        "    X_intercept = np.column_stack((np.ones_like(Y), X1,X2))\n",
        "    beta_hat = np.linalg.inv(X_intercept.T @ X_intercept) @ X_intercept.T @ Y\n",
        "    beta_1.append(beta_hat[1])\n",
        "    beta_2.append(beta_hat[2])"
      ],
      "metadata": {
        "id": "Ke-RFLDaUhzp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Graficar la distribuci√≥n conjunta  de los coeficientes ÀÜŒ≤1 y ÀÜŒ≤2\n",
        "plt.figure(figsize=(7, 4))\n",
        "plt.scatter(beta_1, beta_2,color='#2e92a3', s=40, alpha=0.7)\n",
        "plt.title(r'Distribuci√≥n Conjunta de $\\hat{\\beta_1}$ y $\\hat{\\beta_2}$')\n",
        "plt.xlabel(r'$\\hat{\\beta_1}$')\n",
        "plt.ylabel(r'$\\hat{\\beta_2}$')\n",
        "plt.grid(True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "id": "gSkG8_4dkg7I",
        "outputId": "81626d58-3a29-4ae6-b2a0-cbe18932fc20"
      },
      "execution_count": null
      
    },
    {
      "cell_type": "markdown",
      "source": [
        "Las distribuciones de $\\hat{\\beta_1}$ y $\\hat{\\beta_2}$ est√°n negativamente correlacionadas ya que, como ambas contienen casi la misma informaci√≥n, si insistis en incluir ambas en un modelo, entonces habr√≠a una cantidad casi infinita de combinaciones de b1 y b2 que producen las mismas predicciones."
      ],
      "metadata": {
        "id": "we-UUnaDoH_W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "Desde la perspectiva del modelo:\n",
        "\n",
        "$$Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\varepsilon$$\n",
        "$$Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 (0.5 X_1 + Z) + \\varepsilon$$\n",
        "$$Y = \\beta_0 + (\\beta_1 + 0.5 \\beta_2) X_1 + \\beta_2Z + \\varepsilon$$\n",
        "\n",
        "\n",
        "lo que muestra que los par√°metros $\\beta_1$ y $\\beta_2$ no se pueden separar porque no influencian separadamente a $Y$ (a $\\mu$ en realidad). Solamente su suma $(\\beta_1 + 0.5 \\beta_2)$ influencia $Y$ (el efecto total de $X_1$ est√° tenido en cuenta por $(\\beta_1 + 0.5 \\beta_2)$). As√≠ que lo que termina pasando es que las distintas estimaciones de los coeficientes toman todas las combinaciones posibles de $\\beta_1$ y $\\beta_2$ que hacen que su suma se aproxime a la asociaci√≥n real entre $X_1$ e $Y$.\n",
        "\n",
        "\n",
        "Dado que $X_1$ y $X_2$ est√°n \"compitiendo\" por explicar la misma porci√≥n de la variabilidad de $Y$, cuando uno de los coeficientes aumenta, el otro tiende a disminuir para no duplicar la contribuci√≥n.\n",
        "\n",
        "\n",
        "Lo que est√° respondiendo la regresi√≥n es ‚Äú¬øCu√°l es el valor de conocer cada predictor, una vez que ya conozco los dem√°s?‚Äù (por ejemplo ‚Äú¬øCu√°l es el valor de conocer el largo de cada pierna una vez que ya conozco el largo de la otra?‚Äù).\n",
        "\n",
        "\n",
        "**Las predicciones para $Y$ van a estar bien, pero no te va a decir cu√°l predictor es m√°s importante.**"
      ],
      "metadata": {
        "id": "OXaV6lqCkpEW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "----"
      ],
      "metadata": {
        "id": "yiBtxbUgoT3U"
      }
    }
  ]
}